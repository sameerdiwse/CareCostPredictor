import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.metrics import mean_squared_error
from xgboost import XGBRegressor
import numpy as np


df = pd.read_excel('premiums.xlsx')
df.head()


df.shape


df.columns


df.columns = df.columns.str.replace(' ', '_').str.lower()


df.head()





# Checking for NA values
df.isna().sum()


# dropping rows with na feature values
df.dropna(inplace=True)


df.duplicated().sum()


df.drop_duplicates(inplace=True)
df.duplicated().sum()


df.describe()


# Showing all the negative values from number_of_dependants column
df[df.number_of_dependants<0]['number_of_dependants'].unique()


# making negative values of number_of_dependants positive
df['number_of_dependants'] = abs(df['number_of_dependants'])


df['number_of_dependants'].describe()





# Filtering out all the numeric columns
num_columns = df.select_dtypes(['float64', 'int64']).columns


# Plotting box plot
plt.figure(figsize=(10, 7))

for i, column in enumerate(num_columns):
    plt.subplot(2, 2, i + 1)
    sns.boxplot(x=df[column])
    plt.title(column)

plt.tight_layout()
plt.show()


# Analyzing the outliers of age column
df[df['age']>100]['age'].unique()


df1 = df[df['age']<=100].copy()
df1.describe()


# Analyzing the outliers for 'income_lakhs' feature
# using quantile to set a reasonable threshold

income_quantile_threshold = df1.income_lakhs.quantile(0.999).copy()
print('income quantile threshold is:',income_quantile_threshold)

df2 = df1[df1.income_lakhs<=income_quantile_threshold].copy()
df2.income_lakhs.describe()


# Data Visualization

plt.figure(figsize=(10, 6))

for i, col in enumerate(num_columns):
    plt.subplot(2, 2, i + 1)
    sns.histplot(df2[col])
    plt.title(col)

plt.tight_layout()
plt.show()



categorical_columns = ['income_level', 'employment_status', 'smoking_status', 'bmi_category', 'marital_status', 'gender', 'region', 'medical_history', 'insurance_plan']

for col in categorical_columns:
    print(col, df2[col].unique())

# Something fishy in smoking_status feature


df2['smoking_status'].replace({
    'Smoking=0':'No Smoking',
    'Does Not Smoke':'No Smoking',
    'Not Smoking':'No Smoking'
})

df2['smoking_status'].unique()


gender_percentage = df2['gender'].value_counts(normalize=True)
gender_percentage


fig, axes = plt.subplots(3, 3, figsize=(18, 18))
axes = axes.flatten()
for axx, column in zip(axes, categorical_columns):

    category_counts = df2[column].value_counts(normalize=True) * 100
    
    sns.barplot(x = category_counts.index, y = category_counts.values, ax=axx)

plt.tight_layout()
plt.show()


# For bivariate analysis
crosstab = pd.crosstab(df['income_level'], df['insurance_plan'])
print(crosstab)

crosstab.plot(kind='bar', stacked=True);





df2['medical_history'].unique()


# Let's create a column 'risk_score' for rating the risk according to existing diseases

risk_scores={
    "diabetes":6,
    "heart disease":8,
    "high blood pressure":6,
    "thyroid":5,
    "no disease":0,
    "none":0
}


df2[['disease1', 'disease2']] = df2['medical_history'].str.split(" & ", expand=True).apply(lambda x: x.str.lower())


df2['disease2'] = df2['disease2'].fillna('none')


df2['total_risk_score'] = 0

for disease in ['disease1', 'disease2']:
    df2['total_risk_score'] += df2[disease].map(risk_scores)

# normalizing the risk scores
max_score = df2['total_risk_score'].max()
min_score = df2['total_risk_score'].min()

df2['normalized_risk_score'] = (df2['total_risk_score']-min_score)/(max_score-min_score)

df2['total_risk_score']
df2['disease1'].unique(), df2['disease2'].unique()


# Label encoding for insurance_plan and income level

df2['insurance_plan'] = df2.insurance_plan.map({'Bronze':1, 'Silver':2, 'Gold':3})

df2['income_level'] = df2.income_level.map({'<10L':1, '10L - 25L':2, '25L - 40L':3, '> 40L':4})


# Doing one hot encoding for nominal columns

nominal_cols = ['gender', 'region', 'marital_status', 'bmi_category', 'smoking_status', 'employment_status']

df3 = pd.get_dummies(df2, columns=nominal_cols, drop_first=True, dtype=int)

df3.head()


# Dropping unnecessary columns

df4 = df3.drop(['disease1', 'disease2', 'medical_history', 'total_risk_score'], axis=1)
df4


scaler = MinMaxScaler()

X = df4.drop('annual_premium_amount', axis=1)
y = df4['annual_premium_amount']

cols_to_scale = ['age', 'number_of_dependants', 'income_level', 'income_lakhs', 'insurance_plan']

X[cols_to_scale] = scaler.fit_transform(X[cols_to_scale])

X.describe()


# 'income_level' and 'income_lakhs' seems to have higher VIF, we can drop those columns

X_refined = X.drop(['income_level'], axis=1)
X_refined.shape, y.shape
X_refined.drop(['smoking_status_Smoking=0','smoking_status_Not Smoking','smoking_status_No Smoking'], axis=1, inplace=True)
X_refined.columns





#Splitting the data
X_train, X_test, y_train, y_test = train_test_split(X_refined, y, test_size=0.3, random_state=42)
print('X_train', X_train.shape)
print('X_test',X_test.shape)
print('y_train',y_train.shape)
print('y_test', y_test.shape)






linear_reg_model = LinearRegression()
linear_reg_model.fit(X_train, y_train)
print('linear regression model testing data prediction',linear_reg_model.score(X_test, y_test)) # 100% !!!!!!!! LOL
print('linear regression model training data prediction',linear_reg_model.score(X_train, y_train))
y_pred_linear = linear_reg_model.predict(X_test)
print('mse:',mean_squared_error(y_test, y_pred_linear))






linear_reg_model.coef_, linear_reg_model.feature_names_in_


coef_df = pd.DataFrame({
    'columns': X_train.columns,
    'Coefficients':linear_reg_model.coef_
})

coef_df = coef_df.sort_values(by='Coefficients', ascending=True)
coef_df

plt.barh(coef_df['columns'], coef_df['Coefficients']) # insurance plan will have more impact on the predictions





ridge_model = Ridge(alpha=8)
ridge_model.fit(X_train, y_train)
print('ridge score on testing data:', ridge_model.score(X_test, y_test))
print('ridge score on training data:', ridge_model.score(X_train, y_train))





xgb_model = XGBRegressor(n_estimators=20, max_depth=3)
xgb_model.fit(X_train, y_train)
print('xgb model score for testing data:',xgb_model.score(X_test, y_test))
print('xgb model score for training data:',xgb_model.score(X_train, y_train))





y_pred_xgb = xgb_model.predict(X_test)
mse = mean_squared_error(y_test, y_pred_xgb)
rmse = np.sqrt(mse)
print('this is xgboost mse: ', mse, 'and this is rmse for xgboost',rmse)





xgb_model_regressor = XGBRegressor()
#Parameters Grid
param_selection_grid={
    'n_estimators':[20, 40, 50],
    'learning_rate':[0.1, 0.2, 0.3],
    'max_depth':[3, 4, 5]
}

random_search = RandomizedSearchCV(xgb_model_regressor, param_selection_grid, n_iter=3, cv=3, random_state=42)
random_search.fit(X_train, y_train)
print('randomized search best score: ', random_search.best_score_)
print('best estimator is:', random_search.best_estimator_)
print('best params are:', random_search.best_params_)





coef_df = pd.DataFrame({
    'columns': X_train.columns,
    'Coefficients':xgb_model.feature_importances_
})
# Tree based models use feature_importances, linear model uses coef_

coef_df = coef_df.sort_values(by='Coefficients', ascending=True)
coef_df

plt.barh(coef_df['columns'], coef_df['Coefficients']);






